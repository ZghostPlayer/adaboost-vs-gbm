{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a1ed5b-0716-4652-8d57-7e3d7ad6505b",
   "metadata": {},
   "source": [
    "### Diferenças entre AdaBoosting e GBM (Gradient Boosting Machine)\n",
    "\n",
    "1. **Base Learners**:\n",
    "   - *AdaBoosting*: Usa um único modelo fraco por vez (geralmente árvores de decisão rasas) e ajusta os pesos das observações incorretas em cada iteração.\n",
    "   - *GBM*: Usa múltiplos modelos fracos que corrigem os erros residuais dos modelos anteriores através da soma dos gradientes.\n",
    "\n",
    "2. **Peso das Amostras vs. Erros Residuais**:\n",
    "   - *AdaBoosting*: Foca em ajustar o peso das amostras classificadas incorretamente em cada iteração, dando mais atenção a estas amostras no próximo modelo.\n",
    "   - *GBM*: Ajusta os erros residuais diretamente, minimizando uma função de perda diferenciável (como erro quadrático ou log-loss).\n",
    "\n",
    "3. **Função de Perda**:\n",
    "   - *AdaBoosting*: Trabalha implicitamente para minimizar a exponencial da perda de classificação.\n",
    "   - *GBM*: Oferece flexibilidade para usar diferentes funções de perda, dependendo do problema (regressão ou classificação).\n",
    "\n",
    "4. **Método de Atualização**:\n",
    "   - *AdaBoosting*: Calcula pesos para cada modelo fraco com base em seu desempenho global.\n",
    "   - *GBM*: Constrói modelos sucessivos para reduzir os gradientes dos erros residuais do modelo anterior.\n",
    "\n",
    "5. **Velocidade e Complexidade**:\n",
    "   - *AdaBoosting*: Geralmente mais rápido, pois cada modelo se concentra em pesos ajustados, sem a necessidade de calcular gradientes complexos.\n",
    "   - *GBM*: Tende a ser mais lento, pois a otimização de gradientes exige cálculos mais intensos.\n",
    "\n",
    "### Exemplo de Uso\n",
    "- *AdaBoosting*: Melhor para datasets menores e mais simples onde o foco é classificação rápida.\n",
    "- *GBM*: Mais adequado para problemas complexos, onde o ajuste fino do modelo com gradientes pode trazer maior precisão.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b55b84-b2ba-4ec7-93a4-c614a0de7ebe",
   "metadata": {},
   "source": [
    "### Exemplo Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "853c9cc2-e32f-4cad-a683-39a20a22e7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273024e-4c30-4825-a524-c8bb0c4697cb",
   "metadata": {},
   "source": [
    "### Exemplo de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b98c35e9-7a16-4d1e-8dc9-07f23dd32587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.009154859960321)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267778f2-b3db-4764-b0f8-49ebf4968f21",
   "metadata": {},
   "source": [
    "### 5 Hiperparâmetros Importantes no GBM (Gradient Boosting Machine)\n",
    "\n",
    "1. **`n_estimators`**:\n",
    "   - Representa o número de árvores (ou etapas de boosting) a serem treinadas.\n",
    "   - Valores maiores podem aumentar a capacidade do modelo, mas também aumentam o risco de overfitting.\n",
    "   - Exemplo: `n_estimators=100`\n",
    "\n",
    "2. **`learning_rate`**:\n",
    "   - Taxa de aprendizado que controla o impacto de cada árvore adicionada.\n",
    "   - Valores menores (ex.: `0.01` ou `0.1`) exigem mais árvores para alcançar uma boa performance, mas geralmente resultam em melhor generalização.\n",
    "   - Exemplo: `learning_rate=0.1`\n",
    "\n",
    "3. **`max_depth`**:\n",
    "   - Profundidade máxima de cada árvore individual.\n",
    "   - Controla a complexidade do modelo e previne overfitting em datasets pequenos.\n",
    "   - Exemplo: `max_depth=3`\n",
    "\n",
    "4. **`min_samples_split`**:\n",
    "   - Número mínimo de amostras necessárias para dividir um nó.\n",
    "   - Valores maiores levam a árvores mais simples e podem ajudar a evitar overfitting.\n",
    "   - Exemplo: `min_samples_split=2`\n",
    "\n",
    "5. **`subsample`**:\n",
    "   - Proporção de amostras utilizadas para treinar cada árvore.\n",
    "   - Um valor menor que 1 (ex.: `subsample=0.8`) introduz aleatoriedade e pode melhorar a generalização do modelo.\n",
    "   - Exemplo: `subsample=0.8`\n",
    "\n",
    "### Observação\n",
    "A escolha e o ajuste desses hiperparâmetros devem ser feitos com cuidado, normalmente usando técnicas como validação cruzada ou otimização automatizada (ex.: Grid Search ou Random Search).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e62c9-f965-4b6d-9e4e-ab0d22b6cbaa",
   "metadata": {},
   "source": [
    "### Melhores Parâmetros Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d654df8-fdc0-4fa2-bf01-6b1ce6af61f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Melhores parâmetros encontrados: {'learning_rate': 1.0, 'loss': 'log_loss', 'max_depth': 1, 'n_estimators': 150, 'subsample': 1.0}\n",
      "Melhor precisão (accuracy): 0.9279999999999999\n",
      "Precisão no conjunto de teste: 0.9279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Gerar os dados\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Definir o modelo\n",
    "model = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Definir os hiperparâmetros a serem testados\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'loss': ['log_loss', 'exponential']  # Corrigir o erro nos valores do parâmetro \"loss\"\n",
    "}\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,  # Validação cruzada com 5 folds\n",
    "    n_jobs=-1,  # Usar todos os núcleos disponíveis\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Treinar o GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Resultados\n",
    "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
    "print(\"Melhor precisão (accuracy):\", grid_search.best_score_)\n",
    "\n",
    "# Avaliação no conjunto de teste\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisão no conjunto de teste:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4717f9e9-164a-4420-93f7-cd6b78478deb",
   "metadata": {},
   "source": [
    "### Melhores Parâmetros Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "133de634-5a00-4d76-8db9-d11dd56053c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Melhores parâmetros encontrados: {'learning_rate': 0.2, 'loss': 'absolute_error', 'max_depth': 2, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Melhor MSE (negativo): -5.200984221648046\n",
      "MSE no conjunto de teste: 3.3384694218420607\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Gerar os dados\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "# Definir o modelo\n",
    "model = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "# Definir os hiperparâmetros a serem testados\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'loss': ['squared_error', 'absolute_error']\n",
    "}\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Treinar o GridSearch\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Resultados\n",
    "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
    "print(\"Melhor MSE (negativo):\", grid_search.best_score_)\n",
    "\n",
    "# Avaliação no conjunto de teste\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE no conjunto de teste:\", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b32f51-bcd6-40f4-a9e4-2b9a5d658a3e",
   "metadata": {},
   "source": [
    "### Diferença entre Gradient Boosting Machine (GBM) e Stochastic GBM\n",
    "\n",
    "A maior diferença entre o **Stochastic Gradient Boosting Machine (Stochastic GBM)** e o **Gradient Boosting Machine (GBM)** tradicional está na forma como eles utilizam os dados durante o treinamento de cada árvore.\n",
    "\n",
    "#### Stochastic GBM\n",
    "No Stochastic GBM, uma **amostragem aleatória** (subconjunto) dos dados de treinamento é usada para ajustar cada árvore. Essa amostragem é controlada pelo hiperparâmetro **`subsample`**, que especifica a proporção dos dados a serem usados em cada iteração:\n",
    "\n",
    "- `subsample=1.0`: Comportamento idêntico ao GBM tradicional (todos os dados são usados).\n",
    "- `subsample<1.0`: Apenas uma fração dos dados é usada para treinar cada árvore, introduzindo aleatoriedade.\n",
    "\n",
    "#### Impactos da Diferença\n",
    "\n",
    "1. **Redução do Overfitting**:\n",
    "   - No Stochastic GBM, a introdução da aleatoriedade ajuda a reduzir o risco de overfitting, especialmente em datasets pequenos ou com ruído.\n",
    "   - O GBM tradicional usa todo o conjunto de treinamento, o que pode levar a um ajuste excessivo.\n",
    "\n",
    "2. **Maior Robustez**:\n",
    "   - O uso de amostragem aleatória no Stochastic GBM torna o modelo mais robusto a pequenas variações nos dados, melhorando a generalização.\n",
    "\n",
    "3. **Eficiência Computacional**:\n",
    "   - Como o Stochastic GBM treina cada árvore em um subconjunto dos dados, ele pode ser mais rápido, especialmente em datasets grandes.\n",
    "\n",
    "#### Resumo\n",
    "A introdução da amostragem aleatória no Stochastic GBM é uma ideia inspirada no *bagging* (como no Random Forest), mas aplicada dentro do contexto do boosting. Isso o torna mais eficiente e menos propenso ao overfitting do que o GBM tradicional.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
