# Compara칞칚o AdaBoost vs Gradient Boosting Machine (GBM)

Este reposit칩rio apresenta uma compara칞칚o pr치tica e te칩rica entre **AdaBoost** e **Gradient Boosting Machine (GBM)**, incluindo varia칞칫es como **Stochastic GBM**, utilizando Python e a biblioteca Scikit-learn.

---

## 游늷 Conte칰do
- Diferen칞as conceituais entre AdaBoost e GBM.
- Exemplos de classifica칞칚o e regress칚o.
- Otimiza칞칚o de hiperpar칙metros com **GridSearchCV**.
- Compara칞칚o entre GBM tradicional e **Stochastic GBM**.
- C칩digo totalmente execut치vel em **Jupyter Notebook**.

---

## 游댌 Diferen칞as principais

| Caracter칤stica           | AdaBoost | GBM |
|--------------------------|----------|-----|
| **Base Learners**        | Um modelo fraco por vez, ajustando pesos. | M칰ltiplos modelos que corrigem erros residuais. |
| **Foco do ajuste**       | Pesos das amostras incorretas. | Gradientes dos erros residuais. |
| **Fun칞칚o de perda**      | Minimiza exponencial da perda de classifica칞칚o. | Flex칤vel: regress칚o ou classifica칞칚o. |
| **M칠todo de atualiza칞칚o**| Peso global do modelo fraco. | Redu칞칚o dos gradientes. |
| **Velocidade**           | Geralmente mais r치pido. | Mais lento, mas mais flex칤vel. |

---

## 游 Tecnologias utilizadas
- Python
- Scikit-learn
- NumPy / Pandas
- Matplotlib

---

## 游늵 Resultados de exemplo

**Classifica칞칚o (GradientBoostingClassifier)**  
Acur치cia: ~0.9279 no conjunto de teste.  

**Regress칚o (GradientBoostingRegressor)**  
MSE: ~3.33 no conjunto de teste.  

---
